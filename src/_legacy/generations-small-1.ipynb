{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/miniconda3/envs/dev/lib/python3.11/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from minicons import scorer\n",
    "import torch\n",
    "from transformers import set_seed\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df64a7cafa6545eaab58d3367fb6dca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_name = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm = scorer.IncrementalLMScorer(\"gpt2-xl\", device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_full = \"The nurse, who likes french cuisine, adopted a rescue dog.\"\n",
    "s_vp1 = \"The nurse likes french cuisine.\"\n",
    "s_vp2 = \"The nurse adopted a rescue dog.\"\n",
    "\n",
    "\n",
    "def prompt(sentence):\n",
    "    return f'Ellie said, \"{sentence}\", and Marco replied, \"'\n",
    "\n",
    "\n",
    "def chat_prompt(sentence, tok=tokenizer):\n",
    "    return tok.apply_chat_template(\n",
    "        [   {\"role\": \"system\", \"content\": \"Please respond to the following message as naturally as possible, using a single sentence, as if we were talking to each other. Please keep it short.\"},\n",
    "            { \"role\": \"user\", \"content\": sentence},\n",
    "            {\"role\": \"assistant\", \"content\": \"No, that's not true!\"}],\n",
    "        tokenize=False,\n",
    "        continue_final_message=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Please respond to the following message as naturally as possible, using a single sentence, as if we were talking to each other. Please keep it short.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "blah blah<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "No, that's not true!\n"
     ]
    }
   ],
   "source": [
    "print(chat_prompt(\"blah blah\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded = lm.tokenizer(s_vp1, return_tensors=\"pt\")\n",
    "# encoded = encoded.to(lm.device)\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "encoded = tokenizer([chat_prompt(s_vp1), chat_prompt(s_vp2)], return_tensors=\"pt\", add_special_tokens=False, padding=True)\n",
    "encoded = encoded.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "set_seed(1024)\n",
    "\n",
    "generations = model.generate(\n",
    "    **encoded, \n",
    "    num_return_sequences=10,\n",
    "    # max_length=100,\n",
    "    max_new_tokens = 20,\n",
    "    do_sample=True, \n",
    "    # top_p=None,\n",
    "    # temperature=0.9,\n",
    "    # typical_p=0.2,\n",
    "    # # min_p=0.2,\n",
    "    # # top_p=0.8,\n",
    "    # # top_k = 5000,\n",
    "    # force_words_ids=[[13]],\n",
    "    # stop_strings=[\".\"],\n",
    "    tokenizer=tokenizer,\n",
    "    top_p=None,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    repetition_penalty=1.2\n",
    "    # length_penalty=5.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|end_of_text|>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(128001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = encoded.input_ids.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1], 59)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = torch.arange(len(encoded.input_ids)).tolist()\n",
    "\n",
    "ids, input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generations[, [52]*10:]\n",
    "# [g[52:] for g in generations.split(10)]\n",
    "\n",
    "len(generations.split(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" She actually loves Italian food and can't get enough of pasta carbonara.\", ' She actually loves Italian food and has been trying out new pasta recipes in her free time.', ' The doctor is actually quite fond of Italian food.', ' She actually has a big soft spot for Italian food instead.', ' The doctor actually loves French food and always requests escargot when they travel to Paris.', ' The doctor is actually obsessed with sushi from Japan.', ' She actually has a weak spot for Italian food and pasta dishes.', ' The doctor is actually a huge fan of French cooking and has even written a cookbook on his favorite recipes', '', ' The doctor is more into trying new foods from around the world actually.']\n",
      "[' She actually got a cat last month and named him Whiskers.', ' The vet actually adopted the cute little puppy from the shelter last weekend.', ' The vet actually adopted the pup from the shelter last weekend and named him Max.', ' She actually got her new furry friend from a reputable breeder after years of wanting one.', ' The doctor and his wife actually adopted two adorable puppies from the shelter last weekend!', \" The doctor in our family actually took in the pup after his niece couldn't care for him anymore.\", ' She actually volunteered at an animal shelter and fell in love with one of the dogs there.', ' The doctor actually adopted the rescue cat from the local shelter last week.', ' She actually had one already and loves them both equally.', ' She actually volunteered at the animal shelter where she met her new furry friend.']\n"
     ]
    }
   ],
   "source": [
    "# [x for x in tokenizer.batch_decode(generations[: , input_length:].split(2), skip_special_tokens=True)]\n",
    "# tokenizer.batch_decode(generations,skip_special_tokens=True)\n",
    "\n",
    "for gen in generations[:, input_length:].split([10]*2):\n",
    "    print([x for x in tokenizer.batch_decode(gen, skip_special_tokens=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    40,   3077,   6755,   1364,    596,   8104,  21901,    315,   3623,\n",
       "             80,   8065,  21016,    323,  31054,    484,    370,   2852,    325,\n",
       "              0, 128009],\n",
       "        [    40,   2846,    539,  14792,     11,    358,   3077,   6755,   1077,\n",
       "            436,   2370,    922,    279,  14425,   1056,   1821,    520,    430,\n",
       "            502,  66244],\n",
       "        [    40,   3077,   6755,   1364,    596,   1027,    436,   2370,    922,\n",
       "            430,    502,   8753,  66244,  19441,      0, 128009, 128001, 128001,\n",
       "         128001, 128001],\n",
       "        [    40,   3077,   6755,   1364,    596,   8104,  21901,    315,   3920,\n",
       "            867,    354,    323,  11494,    266,    283,   4618,      0, 128009,\n",
       "         128001, 128001],\n",
       "        [  8100,    596,   2744,    436,   2370,    922,    279,   3920,    867,\n",
       "            354,    520,    430,    502,    293,  15931,  19441,      0, 128009,\n",
       "         128001, 128001],\n",
       "        [  8100,    596,   2744,    436,   2370,    922,    279,   3920,    867,\n",
       "            354,    520,    430,    502,    293,  15931,  19441,      0, 128009,\n",
       "         128001, 128001],\n",
       "        [    40,   3077,   6755,   1364,    596,   8104,  21901,    315,   3623,\n",
       "             80,   8065,  21016,    323,    706,   1027,   3967,    311,   4394,\n",
       "            709,    264],\n",
       "        [    40,   3077,   2744,   1027,  22999,    922,   4560,   1063,    315,\n",
       "            430,  11495,   8753,  30870,    484,    370,   2852,    325,  19724,\n",
       "              0, 128009],\n",
       "        [    40,   3077,   6755,   1364,    596,   8104,  21901,    315,   3623,\n",
       "             80,   8065,  21016,    323,  31054,    484,    370,   2852,    325,\n",
       "              0, 128009],\n",
       "        [    40,   3077,   6755,    430,   3920,    867,    354,    374,    832,\n",
       "            315,   1077,   7075,  26863,      0, 128009, 128001, 128001, 128001,\n",
       "         128001, 128001]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generations[: , input_length:].split([10]*2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
